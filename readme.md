# About the Project
This project will explore three different implementations of the famous LLM , GPT-2;
1. Using huggingface transformers,
2. Using PyTorch, and,
3. From scratch with just matrices.

## Why GPT-2?
1. Simple architecture for understanding,
2. 124M parameter model that can run easily without the need of dedicated GPU,
3. First of its kind so, a good start to know where thing started.